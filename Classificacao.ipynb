{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Linear_Regression as LR\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       893.000000\n",
      "mean      79907.409854\n",
      "std       60880.043823\n",
      "min        9999.000000\n",
      "25%       44500.000000\n",
      "50%       61990.000000\n",
      "75%       90990.000000\n",
      "max      450039.000000\n",
      "Name: price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "a_none = np.array(None)\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "print(data['price'].describe())\n",
    "muito_abaixo = data['price'].describe().loc['25%']\n",
    "abaixo = data['price'].describe().loc['50%']\n",
    "acima = data['price'].describe().loc['75%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajeitando o data set, transformando a variável dependente de continua para discreta\n",
    "data['preco']=pd.cut(x=data['price'], bins=[0,muito_abaixo,abaixo,acima,np.inf], \n",
    "                        labels=[\"muito_abaixo\", \"abaixo\", \"acima\",\"muito_acima\"])\n",
    "\n",
    "l_data = pd.cut(x=data['price'], bins=[0,abaixo,np.inf], \n",
    "                        labels=[0, 1])\n",
    "data = data.drop(['price', 'Unnamed: 0.1',\t'Unnamed: 0'], axis=1)\n",
    "data_c = data[0:500].select_dtypes(include='number')\n",
    "# Adicionando a primeira coluna com 1 para ser usada como constante\n",
    "data_c.insert(0, 'constant', [1]*500)\n",
    "# Ortogonalizando os dados\n",
    "data_ort = LR.gram_schmidt(data_c)\n",
    "data_ort['preco'] = data['preco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando as categorias adicionando colunas para representa-las\n",
    "def one_hot_encode(data_set, column):\n",
    "    data_set = pd.DataFrame(data_set)\n",
    "    one_hot = pd.DataFrame(0, index=range(data_set.shape[0]), columns=data_set[column].unique())\n",
    "    count=0\n",
    "    for i in data_set[column]:\n",
    "        one_hot.loc[count, i] = 1\n",
    "        count+=1\n",
    "    return pd.concat([data_set.drop(column, axis=1), one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>constant</th>\n",
       "      <th>spec_rating</th>\n",
       "      <th>display_size</th>\n",
       "      <th>resolution_width</th>\n",
       "      <th>resolution_height</th>\n",
       "      <th>warranty</th>\n",
       "      <th>abaixo</th>\n",
       "      <th>muito_abaixo</th>\n",
       "      <th>acima</th>\n",
       "      <th>muito_acima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.222706</td>\n",
       "      <td>0.242761</td>\n",
       "      <td>-211.589158</td>\n",
       "      <td>-64.808018</td>\n",
       "      <td>-0.067473</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-8.777294</td>\n",
       "      <td>0.773877</td>\n",
       "      <td>98.434168</td>\n",
       "      <td>10.318448</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.546235</td>\n",
       "      <td>-1.207036</td>\n",
       "      <td>-149.409749</td>\n",
       "      <td>-114.608444</td>\n",
       "      <td>-0.046807</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.777294</td>\n",
       "      <td>-1.071253</td>\n",
       "      <td>249.849608</td>\n",
       "      <td>43.403431</td>\n",
       "      <td>-0.018707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.546235</td>\n",
       "      <td>-1.907036</td>\n",
       "      <td>479.435370</td>\n",
       "      <td>11.919339</td>\n",
       "      <td>-0.033314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>0.546235</td>\n",
       "      <td>0.392964</td>\n",
       "      <td>-123.912878</td>\n",
       "      <td>-43.561846</td>\n",
       "      <td>-0.046983</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>0.546235</td>\n",
       "      <td>-1.207036</td>\n",
       "      <td>-149.409749</td>\n",
       "      <td>-114.608444</td>\n",
       "      <td>-0.046807</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "      <td>0.546235</td>\n",
       "      <td>0.392964</td>\n",
       "      <td>-123.912878</td>\n",
       "      <td>-43.561846</td>\n",
       "      <td>-0.046983</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.777294</td>\n",
       "      <td>-0.948688</td>\n",
       "      <td>241.393452</td>\n",
       "      <td>146.038974</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1</td>\n",
       "      <td>3.222706</td>\n",
       "      <td>-2.016384</td>\n",
       "      <td>-1064.392962</td>\n",
       "      <td>1154.477451</td>\n",
       "      <td>-0.006456</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     constant  spec_rating  display_size  resolution_width  resolution_height  \\\n",
       "0           1     4.222706      0.242761       -211.589158         -64.808018   \n",
       "1           1    -8.777294      0.773877         98.434168          10.318448   \n",
       "2           1     0.546235     -1.207036       -149.409749        -114.608444   \n",
       "3           1    -2.777294     -1.071253        249.849608          43.403431   \n",
       "4           1     0.546235     -1.907036        479.435370          11.919339   \n",
       "..        ...          ...           ...               ...                ...   \n",
       "495         1     0.546235      0.392964       -123.912878         -43.561846   \n",
       "496         1     0.546235     -1.207036       -149.409749        -114.608444   \n",
       "497         1     0.546235      0.392964       -123.912878         -43.561846   \n",
       "498         1    -5.777294     -0.948688        241.393452         146.038974   \n",
       "499         1     3.222706     -2.016384      -1064.392962        1154.477451   \n",
       "\n",
       "     warranty  abaixo  muito_abaixo  acima  muito_acima  \n",
       "0   -0.067473       1             0      0            0  \n",
       "1    0.004980       0             1      0            0  \n",
       "2   -0.046807       0             1      0            0  \n",
       "3   -0.018707       1             0      0            0  \n",
       "4   -0.033314       0             0      1            0  \n",
       "..        ...     ...           ...    ...          ...  \n",
       "495 -0.046983       1             0      0            0  \n",
       "496 -0.046807       1             0      0            0  \n",
       "497 -0.046983       0             1      0            0  \n",
       "498  0.001357       1             0      0            0  \n",
       "499 -0.006456       0             0      0            1  \n",
       "\n",
       "[500 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_class = data_ort.copy()\n",
    "data_ort = one_hot_encode(data_ort, 'preco')\n",
    "data_ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar a regressão linear já programada para o caso de regressão. Como linear regression prevê o valor y de determinado ponto x tentando maximizar a expectativa de y dado o observado y. Podemos concluir o mesmo para o caso de classificação, tendo uma \"probabilidade\" de dado a observação x a mesma pertencer a classe i. (probabilidade entre aspas pois os valores podem ser maior que 1 ou menor que 0, apesar de ao serem somados o resultado é 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32142942,  0.22468941,  0.21744816,  0.23643301,  0.        ],\n",
       "       [ 0.39172497,  0.51835731,  0.21641287, -0.12649515,  1.        ],\n",
       "       [ 0.4065663 ,  0.20588501,  0.24444966,  0.14309903,  0.        ],\n",
       "       ...,\n",
       "       [ 0.34130938,  0.30774029,  0.21715537,  0.13379495,  0.        ],\n",
       "       [ 0.30494999,  0.32648893,  0.23302732,  0.13553375,  1.        ],\n",
       "       [ 0.34665182,  0.10639524,  0.16915904,  0.3777939 ,  3.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para reunir as estimativas de cada least-square\n",
    "def LS_categorical(data, colunas):\n",
    "    # Será salvo cada classe do least-square nessa lista\n",
    "    lista = []\n",
    "    # Será salvo a estimativa dos least square para cada observação\n",
    "    predictions = np.ones([len(data), len(colunas)])\n",
    "    n_var = len(data.columns) - len(colunas)\n",
    "    # Será salvo os parâmetros dos least-square\n",
    "    parametros = np.ones([n_var, len(colunas)])\n",
    "    j = 0\n",
    "    for i in colunas:\n",
    "        y = data[i]\n",
    "        x = data.drop(colunas, axis=1)\n",
    "        lr = LR.LR(x,y)\n",
    "        param = lr.least_square()\n",
    "        parametros[:,j] = param\n",
    "        pred = lr.prediction()\n",
    "        predictions[:,j]=pred\n",
    "        lista.append(lr)\n",
    "        j+=1\n",
    "    # A classe escolhida é aquela em que determinado ls previu o maior valor\n",
    "    select = np.argmax(predictions, axis=1).reshape(len(data),1)\n",
    "    return parametros, np.append(predictions, select, axis=1), lista\n",
    "\n",
    "par, prediction_LS, ls_class = LS_categorical(data_ort, [\"muito_abaixo\", \"abaixo\", \"acima\",\"muito_acima\"])\n",
    "prediction_LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Todas as somas de 'probabilidades' resultam em 1\"\n",
    "np.sum(prediction_LS[:, :-1], axis=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.502"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Acertamos metade das previsões feitas, sendo a média (no chute) 0.25'''\n",
    "# Passando as categorias para int\n",
    "cat_int = data_class['preco'].map({\"muito_abaixo\":0, \"abaixo\":1, \"acima\":2,\"muito_acima\":3})\n",
    "# Havendo 500 dados previsto\n",
    "sum(cat_int.to_numpy() - prediction_LS[:,-1] == 0)/500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos possíveis modelos, criados justamente para o caso de classificação é o linear discriminant analysis, nele é assumido que os dados são:\n",
    " - Gaussian distribution\n",
    " - A covariação entre as diferentes classes é a mesma(isso permite cancelações tornando o modelo linear em x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self, data, prediction_col=None):\n",
    "        self.data = pd.DataFrame(data)\n",
    "        if prediction_col == None:\n",
    "            self.x = self.data.iloc[:, :-1]\n",
    "            self.data.iloc[:,-1] = pd.DataFrame(pd.Categorical(self.data.iloc[:, -1]).codes, columns=['preco'])\n",
    "            self.y = pd.DataFrame(self.data.iloc[:,-1], columns=['preco'])\n",
    "        else:\n",
    "            self.x = self.data.drop(prediction_col, axis=1)\n",
    "            self.data[prediction_col] = pd.DataFrame(pd.Categorical(self.data[prediction_col]).codes, columns=['preco'])\n",
    "            self.y = pd.DataFrame(self.data[prediction_col], columns=[prediction_col])\n",
    "        self.classes = np.array(list(self.y.value_counts(sort=False).values))\n",
    "        self.n_classes = len(self.classes)\n",
    "        # Quantia de classes\n",
    "        self.n_var = len(self.x.columns)\n",
    "\n",
    "    def parametros(self):\n",
    "        self.prior = np.ones([self.n_classes, 1])\n",
    "        for i in range(len(self.prior)):\n",
    "            self.prior[i] = self.classes[i] / sum(self.classes)\n",
    "        self.mean = np.ones([self.n_classes, self.n_var])\n",
    "        for i in range(self.n_classes):\n",
    "            index = self.y[self.y == i].dropna().index\n",
    "            self.mean[i] = np.mean(self.x.loc[index], axis=0) \n",
    "        self.cov = self.x.cov().to_numpy()\n",
    "        \n",
    "    def prediction(self, y=a_none, x=a_none):\n",
    "        if y.all() == None:\n",
    "            # Iremos manter a prob de cada classe com a ultima coluna sendo a classe escolhida\n",
    "            predict = np.ones([len(self.y), self.n_classes+1])\n",
    "            for i in range(len(self.x)):\n",
    "                div = 0\n",
    "                for j in range(self.n_classes):\n",
    "                    div = div + multivariate_normal.pdf(self.x.loc[i], self.mean[j], self.cov)*self.prior[j]\n",
    "\n",
    "                # Usado para identificar a classe com maior probabilidade\n",
    "                prob_aux = 0\n",
    "                for j in range(self.n_classes):  \n",
    "                    # Probabilidade \n",
    "                    prob = multivariate_normal.pdf(self.x.loc[i], self.mean[j], self.cov)*self.prior[j]/div\n",
    "                    if prob>prob_aux:\n",
    "                        prob_aux = prob[0]\n",
    "                        predict[i, -1] = j\n",
    "                    predict[i, j] = prob[0]\n",
    "                    self.predict = predict\n",
    "            return predict\n",
    "        else:\n",
    "            # Iremos manter a prob de cada classe com a ultima coluna sendo a classe escolhida\n",
    "            predict = np.ones([len(y), self.n_classes+1])\n",
    "            for i in range(len(x)):\n",
    "                div = 0\n",
    "                for j in range(self.n_classes):\n",
    "                    div = div + multivariate_normal.pdf(x[i], self.mean[j], self.cov)*self.prior[j]\n",
    "                # Usado para identificar a classe com maior probabilidade\n",
    "                prob_aux = 0\n",
    "                for j in range(self.n_classes):  \n",
    "                    # Probabilidade \n",
    "                    prob = multivariate_normal.pdf(x[i], self.mean[j], self.cov)*self.prior[j]/div      \n",
    "                    if prob>prob_aux:\n",
    "                        prob_aux = prob[0]\n",
    "                        predict[i, -1] = j\n",
    "                    predict[i, j] = prob[0]\n",
    "            return predict\n",
    "\n",
    "    def accurency(self, y_hat=a_none, y=a_none):\n",
    "        if y_hat.all() == None:\n",
    "            acerto=0\n",
    "            for i in range(len(self.y)):\n",
    "                if self.y.loc[i].values[0] - self.predict[i][-1] == 0:\n",
    "                    acerto+=1      \n",
    "            return acerto/len(self.y)\n",
    "        else:\n",
    "            acerto=0\n",
    "            y_hat = pd.DataFrame(y_hat)\n",
    "            y = pd.DataFrame(y)\n",
    "            for i in range(len(y)):\n",
    "                if y.loc[i].values[0] - y_hat.loc[i].values[0] == 0:\n",
    "                    acerto+=1           \n",
    "            return acerto/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de acerto LDA:  0.486\n"
     ]
    }
   ],
   "source": [
    "# Os dados em lda não usam a primeira coluna com 1\n",
    "lda_data = data_class.iloc[:,1:]\n",
    "lda = LDA(lda_data,prediction_col='preco')\n",
    "lda.parametros()\n",
    "pred = lda.prediction()\n",
    "print(\"Porcentagem de acerto LDA: \",lda.accurency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Discriminant Analysis(QDA) é uma modelo similar a LDA, porém por não conter a suposição que a covariância das diferentes classes sejam iguais não há termos que se cancelem, dessa forma permanece ainda na função um termo quadrático em x, tornando esse modelo não linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA:\n",
    "    def __init__(self, data, prediction_col=None):\n",
    "        self.data = pd.DataFrame(data)\n",
    "        if prediction_col == None:\n",
    "            self.x = self.data.iloc[:, :-1]\n",
    "            self.data.iloc[:,-1] = pd.DataFrame(pd.Categorical(self.data.iloc[:, -1]).codes, columns=['preco'])\n",
    "            self.y = pd.DataFrame(self.data.iloc[:,-1], columns=['preco'])\n",
    "        else:\n",
    "            self.x = self.data.drop(prediction_col, axis=1)\n",
    "            self.data[prediction_col] = pd.DataFrame(pd.Categorical(self.data[prediction_col]).codes, columns=['preco'])\n",
    "            self.y = pd.DataFrame(self.data[prediction_col], columns=[prediction_col])\n",
    "        self.classes = np.array(list(self.y.value_counts(sort=False).values))\n",
    "        self.n_classes = len(self.classes)\n",
    "        # Quantia de classes\n",
    "        self.n_var = len(self.x.columns)\n",
    "\n",
    "    def parametros(self):\n",
    "        self.prior = np.ones([self.n_classes, 1])\n",
    "        for i in range(len(self.prior)):\n",
    "            self.prior[i] = self.classes[i] / sum(self.classes)\n",
    "        self.cov = np.ones([self.n_classes, self.n_var, self.n_var])\n",
    "        self.mean = np.ones([self.n_classes, self.n_var])\n",
    "        for i in range(self.n_classes):\n",
    "            index = self.y[self.y == i].dropna().index\n",
    "            self.mean[i] = np.mean(self.x.loc[index], axis=0)\n",
    "            self.cov[i] = self.x.loc[index].cov()\n",
    "\n",
    "    def prediction(self, x=a_none):\n",
    "        if x.all() == None:\n",
    "            # Iremos manter a prob de cada classe com a ultima coluna sendo a classe escolhida\n",
    "            predict = np.ones([len(self.y), self.n_classes+1])\n",
    "            for i in range(len(self.x)):\n",
    "                div = 0\n",
    "                for j in range(self.n_classes):\n",
    "                    div = div + multivariate_normal.pdf(self.x.loc[i], self.mean[j], self.cov[j])*self.prior[j]\n",
    "                # Usado para identificar a classe com maior probabilidade\n",
    "                prob_aux = 0\n",
    "                for j in range(self.n_classes):  \n",
    "                    # Probabilidade \n",
    "                    prob = multivariate_normal.pdf(self.x.loc[i], self.mean[j], self.cov[j])*self.prior[j]/div\n",
    "                    if prob>prob_aux:\n",
    "                        prob_aux = prob[0]\n",
    "                        predict[i, -1] = j\n",
    "                    predict[i, j] = prob[0]\n",
    "                    self.predict = predict\n",
    "            return predict\n",
    "        else:\n",
    "            # Iremos manter a prob de cada classe com a ultima coluna sendo a classe escolhida\n",
    "            predict = np.ones([len(y), self.n_classes+1])\n",
    "            for i in range(len(x)):\n",
    "                div = 0\n",
    "                for j in range(self.n_classes):\n",
    "                    div = div + multivariate_normal.pdf(x[i], self.mean[j], self.cov[j])*self.prior[j]\n",
    "                # Usado para identificar a classe com maior probabilidade\n",
    "                prob_aux = 0\n",
    "                for j in range(self.n_classes):  \n",
    "                    # Probabilidade \n",
    "                    prob = multivariate_normal.pdf(x[i], self.mean[j], self.cov[j])*self.prior[j]/div\n",
    "                    if prob>prob_aux:\n",
    "                        prob_aux = prob[0]\n",
    "                        predict[i, -1] = j\n",
    "                    predict[i, j] = prob[0]\n",
    "            return predict\n",
    "        \n",
    "    def accurency(self, y_hat=a_none, y=a_none):\n",
    "        if y_hat.all() == None:\n",
    "            acerto=0\n",
    "            for i in range(len(self.y)):\n",
    "                if self.y.loc[i].values[0] - self.predict[i][-1] == 0:\n",
    "                    acerto+=1\n",
    "            return acerto/len(self.y)\n",
    "        else:\n",
    "            acerto=0\n",
    "            y_hat = pd.DataFrame(y_hat)\n",
    "            y = pd.DataFrame(y)\n",
    "            for i in range(len(y)):\n",
    "                if y.loc[i].values[0] - y_hat.loc[i].values[0] == 0:\n",
    "                    acerto+=1           \n",
    "            return acerto/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de acerto QDA:  0.45\n"
     ]
    }
   ],
   "source": [
    "qda = QDA(lda_data,prediction_col='preco')\n",
    "qda.parametros()\n",
    "pred_qda = qda.prediction()\n",
    "print(\"Porcentagem de acerto QDA: \",qda.accurency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, data, prediction_col=None):\n",
    "        self.data = pd.DataFrame(data)\n",
    "        if prediction_col == None:\n",
    "            self.x = self.data.iloc[:, :-1].to_numpy()\n",
    "            self.data.iloc[:,-1] = pd.DataFrame(pd.Categorical(self.data.iloc[:, -1]).codes, columns=['preco'])\n",
    "            self.y = pd.DataFrame(self.data.iloc[:,-1], columns=['preco']).to_numpy()            \n",
    "        else:\n",
    "            self.x = self.data.drop(prediction_col, axis=1).to_numpy()\n",
    "            self.data[prediction_col] = pd.DataFrame(pd.Categorical(self.data[prediction_col]).codes, columns=['preco'])\n",
    "            self.y = pd.DataFrame(self.data[prediction_col], columns=[prediction_col]).to_numpy()\n",
    "        self.classes = np.array(list(np.unique(self.y)))\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "    def treino(self):\n",
    "        self.parametros = np.array([0]*(self.x.shape[1])).reshape(self.x.shape[1])\n",
    "        p1_passado = 0\n",
    "        p1=np.inf\n",
    "        while abs(np.sum(p1_passado - p1)) > 0.0001:\n",
    "            # Manter as mudanças nas estimativas\n",
    "            p1_passado = p1\n",
    "            self.parametros = self.parametros.reshape(self.x.shape[1], 1) \n",
    "            # Calculando a probabilidade de cada observação\n",
    "            numerador = np.exp(self.x.dot(self.parametros))\n",
    "            denominador = (1 + np.exp(self.x.dot(self.parametros)))\n",
    "            p1 = (numerador/denominador).reshape(self.x.shape[0], 1)\n",
    "            p2 = 1-p1\n",
    "            # Iniciando as matrizes que irão manter as derivadas 1 e 2\n",
    "            der_b = np.zeros([1, self.x.shape[1]])\n",
    "            der_b2 = np.zeros([self.x.shape[1], self.x.shape[1]])           \n",
    "            for i in range(self.x.shape[0]):\n",
    "                der_b2 = der_b2 + (self.x[i].reshape(self.x.shape[1],1).dot(self.x[i].reshape(1,self.x.shape[1])))*p1[i]*p2[i]\n",
    "                der_b = der_b + self.x[i]*((self.y[i] - p1[i]))  \n",
    "            der_b2 *=-1\n",
    "            b_new = self.parametros - np.linalg.inv(der_b2).dot(der_b.T)\n",
    "            self.parametros = b_new\n",
    "        return self.parametros "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
